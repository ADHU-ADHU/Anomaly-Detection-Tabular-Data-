{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "# Load the train.mat file\n",
    "mat_file_path = \"train.mat\"\n",
    "data = scipy.io.loadmat(mat_file_path)\n",
    "\n",
    "# Display the keys in the loaded data\n",
    "data.keys()\n"
   ],
   "id": "4038a321a3a3c9df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract the Train_data object\n",
    "train_data = data['Train_data']\n",
    "\n",
    "# Check the type and shape\n",
    "type(train_data), train_data.shape\n"
   ],
   "id": "80ae2e3275926b2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_data[0].size",
   "id": "ee4b4e4bdd25ba43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_data.shape[1]",
   "id": "8f72a9c1e2830ac9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract the first system (system 0)\n",
    "system_0 = train_data[0, 0]\n",
    "\n",
    "# Check what fields or attributes are available in this system's structure\n",
    "system_0.dtype.names\n"
   ],
   "id": "4c4b9656c99546d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "system_0",
   "id": "cc4a9715d5e2b8fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract trajectory and label for system 0\n",
    "trajectory_0 = system_0['trajectory']\n",
    "label_0 = system_0['Label']\n",
    "\n",
    "# Check shapes of both arrays\n",
    "trajectory_0.shape, label_0.shape\n"
   ],
   "id": "7c020db53b84e0f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trajectory_0[0,0].shape",
   "id": "7d9ba0b30eb845d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "label_0[0].shape\n",
   "id": "7f2d3451463c1fe6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract trajectory of component 0\n",
    "component_0_traj = trajectory_0[0, 0]  # 1st row, 1st component\n",
    "\n",
    "# Check shape and type\n",
    "type(component_0_traj), component_0_traj.shape\n"
   ],
   "id": "ac3d9731b6284f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_normal_data = []\n",
    "\n",
    "for i in range(train_data.shape[1]):\n",
    "    system = train_data[0, i]\n",
    "    trajectories = system['trajectory']  # shape (1, 4)\n",
    "    labels = system['Label']            # shape (4, 1000)\n",
    "    \n",
    "    for j in range(4):  # For each of the 4 trajectories per system\n",
    "        traj = trajectories[0, j]       # shape (10, 1000)\n",
    "        label = labels[j]               # shape (1000,)\n",
    "        \n",
    "        # Select only columns (time steps) where label is 0\n",
    "        normal_indices = label == 0\n",
    "        normal_data = traj[:, normal_indices]  # shape (10, N_normal)\n",
    "        \n",
    "        # Transpose to (N_normal, 10)\n",
    "        normal_data = normal_data.T\n",
    "        all_normal_data.append(normal_data)\n",
    "\n",
    "# Stack all normal data\n",
    "X_train = np.vstack(all_normal_data) \n",
    "print(\"Shape of normal trajectory: \" , X_train.shape)"
   ],
   "id": "dd8220a71c509a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "\n",
   "id": "c1a1859e05841084",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Collect all anomalous samples\n",
    "all_anomaly_data = []\n",
    "\n",
    "for i in range(train_data.shape[1]):\n",
    "    system = train_data[0, i]\n",
    "    trajectories = system['trajectory']  # shape (1, 4)\n",
    "    labels = system['Label']            # shape (4, 1000)\n",
    "    \n",
    "    for j in range(4):  # For each of the 4 trajectories per system\n",
    "        traj = trajectories[0, j]       # shape (10, 1000)\n",
    "        label = labels[j]               # shape (1000,)\n",
    "        \n",
    "        # Select only columns (time steps) where label is 1\n",
    "        anomaly_indices = label == 1\n",
    "        anomaly_data = traj[:, anomaly_indices]  # shape (10, N_anomaly)\n",
    "        \n",
    "        # Transpose to (N_anomaly, 10)\n",
    "        anomaly_data = anomaly_data.T\n",
    "        all_anomaly_data.append(anomaly_data)\n",
    "\n",
    "# Stack all anomaly data\n",
    "X_anomaly = np.vstack(all_anomaly_data)  # Final shape: (N_total_anomaly, 10)\n",
    "print(\"Anomaly data shape:\", X_anomaly.shape)"
   ],
   "id": "625bd37a34bb373d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ],
   "id": "57d95664a9cd2855",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device \", device)"
   ],
   "id": "b73e2bb3bbab1849",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train/test split\n",
    "train_df, test_df = train_test_split(\n",
    "    X_train,  # Ensure we only use the specified features\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ],
   "id": "329d248c8922d919",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Scale data with standard scaler\n",
    "#scaler = StandardScaler()\n",
    "#train_df_scaled = scaler.fit_transform(train_df)\n",
    "#test_df_scaled = scaler.transform(test_df)\n",
    "#anomaly_scaled = scaler.transform(X_anomaly)  \n",
    "\n",
    "# Scale data with min-max scaler\n",
    "#scaler = MinMaxScaler()\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "train_df_scaled = scaler.fit_transform(train_df)\n",
    "test_df_scaled = scaler.transform(test_df)\n",
    "anomaly_scaled = scaler.transform(X_anomaly)  "
   ],
   "id": "c50a911fcc63743a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"shape of train data \", train_df_scaled.shape)\n",
    "print(\"shape of test data \", test_df_scaled.shape)\n",
    "print(\"shape of anomaly data \", anomaly_scaled.shape)"
   ],
   "id": "d551dd6033e4dd31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert to PyTorch tensors\n",
    "train_data_tensor = torch.FloatTensor(train_df_scaled).to(device)\n",
    "test_data_tensor = torch.FloatTensor(test_df_scaled).to(device)\n",
    "anomaly_data_tensor = torch.FloatTensor(anomaly_scaled).to(device)\n",
    "\n",
    "\n",
    "# Step 3: GAN Architecture\n",
    "\n",
    "\n",
    "data_dim = train_data_tensor.shape[1]\n",
    "data_dim"
   ],
   "id": "28eb971fe201fee4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "latent_dim = 8\n",
    "\n",
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self, data_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(data_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "        )\n",
    "        self.mu_layer = nn.Linear(16, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(16, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        mu = self.mu_layer(x)\n",
    "        logvar = self.logvar_layer(x)\n",
    "        return mu, logvar\n",
    "\n",
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self, data_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, data_dim),\n",
    "            nn.Tanh()  # because the data is scaled between -1 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std\n"
   ],
   "id": "b8d32f3ae4deffa5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c655e5c91492b44f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 4: Instantiate models\n",
    "encoder = VAE_Encoder(data_dim, latent_dim).to(device)\n",
    "decoder = VAE_Decoder(data_dim, latent_dim).to(device)\n",
    "\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "beta = 100\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    recon_loss = loss_function(recon_x, x)  # or L1\n",
    "    # KL Divergence between the posterior and the standard normal prior\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0) \n",
    "    return recon_loss + kl_div * beta, recon_loss, kl_div\n",
    "\n",
    "\n",
    "\n",
    "#lr = 0.0001"
   ],
   "id": "17ec3647252d6e16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#opt_g = optim.Adam(generator.parameters(), lr=lr)\n",
    "#opt_d = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "opt_enc = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Training loop\n",
    "epochs = 5\n",
    "batch_size = 512\n"
   ],
   "id": "f18dee8d74f5f7db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "dataset = TensorDataset(train_data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ],
   "id": "6fb73b9a3faf7f26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Setup for checkpointing ===\n",
    "checkpoint_path = 'checkpoints/'\n",
    "checkpoint_file = os.path.join(checkpoint_path, 'Pytorch_Variational_Autoencoder.pth')\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "start_epoch = 0\n",
    "best_loss_AE = float('inf')\n",
    "\n",
    "# === Try to load checkpoint if it exists ===\n",
    "if os.path.exists(checkpoint_file):\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "    opt_enc.load_state_dict(checkpoint['opt_enc_state_dict'])\n",
    "\n",
    "\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_loss_AE = checkpoint['best_loss_AE']\n",
    "\n",
    "    print(f\"âœ… Loaded checkpoint from epoch {start_epoch - 1}\")\n",
    "\n",
    "# === Lists to store losses ===\n",
    "\n",
    "losses_rec = []\n",
    "losses_kl = []\n",
    "losses_total = []\n",
    "\n",
    "for epoch in tqdm(range(start_epoch, epochs)):\n",
    "\n",
    "    epoch_total_loss = 0.0\n",
    "    epoch_recon_loss = 0.0\n",
    "    epoch_kl_loss = 0.0\n",
    "    \n",
    "\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch_idx, (real_data,) in enumerate(dataloader):\n",
    "        num_batches += 1\n",
    "        real_data = real_data.to(device)\n",
    "\n",
    "        # === Train Autoencoder ===\n",
    "        mu, logvar = encoder(real_data)\n",
    "        z = reparameterize(mu, logvar)\n",
    "        recon_data = decoder(z)\n",
    "        loss, recon_loss, kl_div = vae_loss(recon_data, real_data, mu, logvar)\n",
    "        \n",
    "        opt_enc.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_enc.step()\n",
    "\n",
    "        # === Accumulate losses ===\n",
    "        epoch_total_loss += loss.item()\n",
    "        epoch_recon_loss += recon_loss.item()\n",
    "        epoch_kl_loss += kl_div.item()\n",
    "        \n",
    "\n",
    "\n",
    "    # === Average Epoch Losses ===\n",
    "    avg_total_loss = epoch_total_loss / num_batches\n",
    "    avg_recon_loss = epoch_recon_loss / num_batches\n",
    "    avg_kl_loss = epoch_kl_loss / num_batches\n",
    "\n",
    "    losses_rec.append(avg_total_loss)\n",
    "    losses_kl.append(avg_recon_loss)\n",
    "    losses_total.append(avg_total_loss)\n",
    "    \n",
    "\n",
    "\n",
    "    # === Save checkpoint every N epochs or on best loss improvement ===\n",
    "    if epoch % 1 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'encoder_state_dict': encoder.state_dict(),\n",
    "            'decoder_state_dict': decoder.state_dict(),\n",
    "\n",
    "            'opt_enc_state_dict': opt_enc.state_dict(),\n",
    "\n",
    "            'best_loss_AE': best_loss_AE,\n",
    "        }, checkpoint_file)\n",
    "        print(f\"ðŸ’¾ Checkpoint saved at epoch {epoch} with total loss : {avg_total_loss:.4f}\")\n",
    "\n",
    "    # === Early Stopping ===\n",
    "    if avg_total_loss < 0.0005:\n",
    "        print(f\"â›” Early stopping at epoch {epoch} as VAE total loss reached {avg_total_loss:.4f}\")\n",
    "        break\n",
    "\n",
    "    # === Logging ===\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"ðŸ“˜ Epoch {epoch}: Total = {avg_total_loss:.4f} | Recon = {avg_recon_loss:.4f} | KL = {avg_kl_loss:.4f}\")\n",
    "        \n"
   ],
   "id": "54fedfc6c1651344",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(y=losses_total, mode='lines', name='Total loss'))\n",
    "fig.add_trace(go.Scatter(y=losses_rec, mode='lines', name='Reconstruction Loss'))\n",
    "fig.add_trace(go.Scatter(y=losses_kl, mode='lines', name='KL Divergence'))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Training Losses Over Epochs',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Loss',\n",
    "    legend_title='Loss Type',\n",
    "    template='plotly_white',\n",
    "\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ],
   "id": "45855a2c5b6826d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a6e602decd895edc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Discriminator Loss\n",
    "fig_d = go.Figure()\n",
    "fig_d.add_trace(go.Scatter(y=losses_total, mode='lines', name='Total Loss'))\n",
    "fig_d.update_layout(title='Total Loss', xaxis_title='Epoch', yaxis_title='Loss', template='plotly_dark')\n",
    "fig_d.show()\n",
    "\n",
    "# Generator Loss\n",
    "fig_g = go.Figure()\n",
    "fig_g.add_trace(go.Scatter(y=losses_rec, mode='lines', name='Reconstruction Loss'))\n",
    "fig_g.update_layout(title='Reconstruction Loss', xaxis_title='Epoch', yaxis_title='Loss', template='plotly_dark')\n",
    "fig_g.show()\n",
    "\n",
    "# Reconstruction Loss\n",
    "fig_rec = go.Figure()\n",
    "fig_rec.add_trace(go.Scatter(y=losses_kl, mode='lines', name='KL Divergence'))\n",
    "fig_rec.update_layout(title='KL Divergence', xaxis_title='Epoch', yaxis_title='Loss', template='plotly_dark')\n",
    "fig_rec.show()\n",
    "\n",
    "\n"
   ],
   "id": "ac32a4e6f2db0147",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "11d836a3890add7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "checkpoint = torch.load('checkpoints/Pytorch_Variational_Autoencoder.pth')\n",
    "\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n"
   ],
   "id": "d16e7cfe2effbc07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e26a5b9a7680cea2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_vae_outputs(data_tensor):\n",
    "    with torch.no_grad():\n",
    "        mu, logvar = encoder(data_tensor)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        \n",
    "        recon_x = decoder(z)\n",
    "        \n",
    "        recon_loss =  torch.mean((data_tensor - recon_x) ** 2, dim=1)  # or L1\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "        total_loss = recon_loss + kl_loss\n",
    "\n",
    "        return total_loss.cpu().numpy(), recon_loss.cpu().numpy(), kl_loss.cpu().numpy(), mu.cpu().numpy(), logvar.cpu().numpy(), z.cpu().numpy()"
   ],
   "id": "71264f3d8828da5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d8b670bd0c8e9f8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_loss_train, recon_loss_train, kl_loss_train, mu_train, logvar_train, z_train= compute_vae_outputs(train_data_tensor)\n",
    "total_loss_test, recon_loss_test, kl_loss_test, mu_test, logvar_test, z_test= compute_vae_outputs(test_data_tensor)\n",
    "total_loss_anomaly, recon_loss_anomaly, kl_loss_anomaly, mu_anomaly, logvar_anomaly, z_anomaly= compute_vae_outputs(anomaly_data_tensor)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "4224c50f1561c2b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(total_loss_train)",
   "id": "8e6413cf622f45d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(kl_loss_train)",
   "id": "2aace366a4d3593d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(recon_loss_train)",
   "id": "1e204c2da6e84a9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "mu_train[0:5]",
   "id": "13366ecf8779fdbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Score': list(total_loss_train) + list(total_loss_test) + list(total_loss_anomaly),\n",
    "    'Set': (['Train'] * len(total_loss_train)) +\n",
    "           (['Test'] * len(total_loss_test)) +\n",
    "           (['Anomaly'] * len(total_loss_anomaly))\n",
    "})\n",
    "\n",
    "fig = px.box(df, x='Set', y='Score', title='Total loss', template='plotly_dark')\n",
    "fig.show()\n"
   ],
   "id": "c6f60f4e3f98ae74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "acb9c44bbf34900e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Score': list(recon_loss_train) + list(recon_loss_test) + list(recon_loss_anomaly),\n",
    "    'Set': (['Train'] * len(total_loss_train)) +\n",
    "           (['Test'] * len(total_loss_test)) +\n",
    "           (['Anomaly'] * len(total_loss_anomaly))\n",
    "})\n",
    "\n",
    "fig = px.box(df, x='Set', y='Score', title='Reconstruction loss', template='plotly_dark')\n",
    "fig.show()\n"
   ],
   "id": "5a3a942e45ed8199",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6bba7ff9d70ba3f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Score': list(kl_loss_train) + list(kl_loss_test) + list(kl_loss_anomaly),\n",
    "    'Set': (['Train'] * len(total_loss_train)) +\n",
    "           (['Test'] * len(total_loss_test)) +\n",
    "           (['Anomaly'] * len(total_loss_anomaly))\n",
    "})\n",
    "\n",
    "fig = px.box(df, x='Set', y='Score', title='KL Divergence', template='plotly_dark')\n",
    "fig.show()\n"
   ],
   "id": "dbdd2ef00156850b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fa342d33cd1a2476",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def downsample(arr, n=5000):\n",
    "    idx = np.random.choice(len(arr), size=min(n, len(arr)), replace=False)\n",
    "    return arr[idx]\n",
    "\n",
    "# downsampling the data\n",
    "latent_train = downsample(mu_train)\n",
    "latent_test = downsample(mu_test)\n",
    "latent_anomaly = downsample(mu_anomaly)\n",
    "\n",
    "X = np.vstack([latent_train, latent_test, latent_anomaly])\n",
    "labels = (['train'] * len(latent_train) +\n",
    "          ['test'] * len(latent_test) +\n",
    "          ['anomaly'] * len(latent_anomaly))\n"
   ],
   "id": "50c6b0fff383eeaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7779fe284aaa7070",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=5, n_jobs=-1)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(X_2d, columns=['x', 'y'])\n",
    "df['label'] = labels\n",
    "\n",
    "fig = px.scatter(df, x='x', y='y', color='label',\n",
    "                 title='t-SNE Visualization of Encoder Latent Space Z - mean')\n",
    "fig.show()\n",
    "\n",
    "tsne = TSNE(n_components=3, perplexity=30, random_state=5, n_jobs=-1)\n",
    "X_3d = tsne.fit_transform(X)\n",
    "\n",
    "df_3d = pd.DataFrame(X_3d, columns=['x', 'y', 'z'])\n",
    "df_3d['label'] = labels\n",
    "\n",
    "fig_3d = px.scatter_3d(df_3d, x='x', y='y', z='z', color='label',\n",
    "                       title='3D t-SNE Visualization of Encoder Latent Space Z - mean')\n",
    "fig_3d.update_traces(marker=dict(size=3))\n",
    "fig_3d.show()\n"
   ],
   "id": "b32f98f3b3d03a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b45df8b279f0e83b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# downsampling the data\n",
    "latent_train = downsample(logvar_train)\n",
    "latent_test = downsample(logvar_test)\n",
    "latent_anomaly = downsample(logvar_anomaly)\n",
    "\n",
    "X = np.vstack([latent_train, latent_test, latent_anomaly])\n",
    "labels = (['train'] * len(latent_train) +\n",
    "          ['test'] * len(latent_test) +\n",
    "          ['anomaly'] * len(latent_anomaly))\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=5,n_jobs=-1)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "\n",
    "df = pd.DataFrame(X_2d, columns=['x', 'y'])\n",
    "df['label'] = labels\n",
    "\n",
    "fig = px.scatter(df, x='x', y='y', color='label',\n",
    "                 title='t-SNE Visualization of Encoder Latent Space Z-log-var')\n",
    "fig.show()\n",
    "\n",
    "tsne = TSNE(n_components=3, perplexity=30, random_state=5, n_jobs=-1)\n",
    "X_3d = tsne.fit_transform(X)\n",
    "\n",
    "df_3d = pd.DataFrame(X_3d, columns=['x', 'y', 'z'])\n",
    "df_3d['label'] = labels\n",
    "\n",
    "fig_3d = px.scatter_3d(df_3d, x='x', y='y', z='z', color='label',\n",
    "                       title='3D t-SNE Visualization of Encoder Latent Space Z-log-var')\n",
    "fig_3d.update_traces(marker=dict(size=3))\n",
    "fig_3d.show()\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "6d53053058f9a4af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "654e464f772f65e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# downsampling the data\n",
    "latent_train = downsample(z_train)\n",
    "latent_test = downsample(z_test)\n",
    "latent_anomaly = downsample(z_anomaly)\n",
    "\n",
    "X = np.vstack([latent_train, latent_test, latent_anomaly])\n",
    "labels = (['train'] * len(latent_train) +\n",
    "          ['test'] * len(latent_test) +\n",
    "          ['anomaly'] * len(latent_anomaly))\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=5, n_jobs=-1)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "\n",
    "df = pd.DataFrame(X_2d, columns=['x', 'y'])\n",
    "df['label'] = labels\n",
    "\n",
    "fig = px.scatter(df, x='x', y='y', color='label',\n",
    "                 title='t-SNE Visualization of Encoder Latent Space Z')\n",
    "fig.show()\n",
    "\n",
    "tsne = TSNE(n_components=3, perplexity=30, random_state=5, n_jobs=-1)\n",
    "X_3d = tsne.fit_transform(X)\n",
    "\n",
    "df_3d = pd.DataFrame(X_3d, columns=['x', 'y', 'z'])\n",
    "df_3d['label'] = labels\n",
    "\n",
    "fig_3d = px.scatter_3d(df_3d, x='x', y='y', z='z', color='label',\n",
    "                       title='3D t-SNE Visualization of Encoder Latent Space Z')\n",
    "fig_3d.update_traces(marker=dict(size=3))\n",
    "fig_3d.show()\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "2e3159086d2e5a75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3b5c0957009d7afb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import umap\n",
    "latent_train = downsample(mu_train)\n",
    "latent_test = downsample(mu_test)\n",
    "latent_anomaly = downsample(mu_anomaly)\n",
    "\n",
    "X = np.vstack([latent_train, latent_test, latent_anomaly])\n",
    "labels = (['train'] * len(latent_train) +\n",
    "          ['test'] * len(latent_test) +\n",
    "          ['anomaly'] * len(latent_anomaly))\n",
    "\n",
    "umap_2d = umap.UMAP(n_components=2, n_jobs= -1)\n",
    "X_2d = umap_2d.fit_transform(X)\n",
    "\n",
    "df_2d = pd.DataFrame(X_2d, columns=['x', 'y'])\n",
    "df_2d['label'] = labels\n",
    "\n",
    "# 2D Plot\n",
    "fig_2d = px.scatter(df_2d, x='x', y='y', color='label',\n",
    "                    title=\"UMAP 2D Latent Visualization of Encoder Latent Space Z mean\",\n",
    "                    opacity=0.7)\n",
    "fig_2d.show()\n",
    "\n",
    "# Step 3: UMAP to 3D\n",
    "umap_3d = umap.UMAP(n_components=3,n_jobs= -1)\n",
    "X_3d = umap_3d.fit_transform(X)\n",
    "\n",
    "df_3d = pd.DataFrame(X_3d, columns=['x', 'y', 'z'])\n",
    "df_3d['label'] = labels\n",
    "\n",
    "# 3D Plot\n",
    "fig_3d = px.scatter_3d(df_3d, x='x', y='y', z='z', color='label',\n",
    "                       title=\"UMAP 3D Latent Visualization of Encoder Latent Space Z mean\",\n",
    "                       opacity=0.7)\n",
    "fig_3d.update_traces(marker=dict(size=3))\n",
    "fig_3d.show()"
   ],
   "id": "30a165b5c98c7879",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7afcf23dcfc69ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import umap\n",
    "latent_train = mu_train\n",
    "latent_test = mu_test\n",
    "latent_anomaly = mu_anomaly\n",
    "\n",
    "X = np.vstack([latent_train, latent_test, latent_anomaly])\n",
    "labels = (['train'] * len(latent_train) +\n",
    "          ['test'] * len(latent_test) +\n",
    "          ['anomaly'] * len(latent_anomaly))\n",
    "\n",
    "umap_2d = umap.UMAP(n_components=2, n_jobs= -1)\n",
    "X_2d = umap_2d.fit_transform(X)\n",
    "\n",
    "df_2d = pd.DataFrame(X_2d, columns=['x', 'y'])\n",
    "df_2d['label'] = labels\n",
    "\n",
    "# 2D Plot\n",
    "fig_2d = px.scatter(df_2d, x='x', y='y', color='label',\n",
    "                    title=\"UMAP 2D Latent Visualization of Encoder Latent Space Z mean\",\n",
    "                    opacity=0.7)\n",
    "fig_2d.show()\n",
    "\n",
    "# Step 3: UMAP to 3D\n",
    "umap_3d = umap.UMAP(n_components=3,n_jobs= -1)\n",
    "X_3d = umap_3d.fit_transform(X)\n",
    "\n",
    "df_3d = pd.DataFrame(X_3d, columns=['x', 'y', 'z'])\n",
    "df_3d['label'] = labels\n",
    "\n",
    "# 3D Plot\n",
    "fig_3d = px.scatter_3d(df_3d, x='x', y='y', z='z', color='label',\n",
    "                       title=\"UMAP 3D Latent Visualization of Encoder Latent Space Z mean\",\n",
    "                       opacity=0.7)\n",
    "fig_3d.update_traces(marker=dict(size=3))\n",
    "fig_3d.show()"
   ],
   "id": "9b0b5881fe459df4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a9bce8f1834b7de8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "latent_train = mu_train\n",
    "latent_test = mu_test\n",
    "latent_anomaly = mu_anomaly\n",
    "\n",
    "X = np.vstack([latent_train, latent_test, latent_anomaly])\n",
    "labels = (['train'] * len(latent_train) +\n",
    "          ['test'] * len(latent_test) +\n",
    "          ['anomaly'] * len(latent_anomaly))\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=5, n_jobs=-1)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(X_2d, columns=['x', 'y'])\n",
    "df['label'] = labels\n",
    "\n",
    "fig = px.scatter(df, x='x', y='y', color='label',\n",
    "                 title='t-SNE Visualization of Encoder Latent Space Z - mean')\n",
    "fig.show()\n",
    "\n",
    "tsne = TSNE(n_components=3, perplexity=30, random_state=5, n_jobs=-1)\n",
    "X_3d = tsne.fit_transform(X)\n",
    "\n",
    "df_3d = pd.DataFrame(X_3d, columns=['x', 'y', 'z'])\n",
    "df_3d['label'] = labels\n",
    "\n",
    "fig_3d = px.scatter_3d(df_3d, x='x', y='y', z='z', color='label',\n",
    "                       title='3D t-SNE Visualization of Encoder Latent Space Z - mean')\n",
    "fig_3d.update_traces(marker=dict(size=3))\n",
    "fig_3d.show()"
   ],
   "id": "5122eeca3314040a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c10e3344b95ca154",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, precision_recall_curve, auc,\n",
    "    accuracy_score, f1_score, precision_score, recall_score\n",
    ")\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Anomaly scores\n",
    "scores_train_2 = np.array(kl_loss_train)\n",
    "scores_test_2 = np.array(kl_loss_test)\n",
    "scores_anomaly_2 = np.array(kl_loss_anomaly)\n",
    "\n",
    "# Test data is normal (label 0), anomaly is (label 1)\n",
    "y_true = np.concatenate([np.zeros_like(scores_test_2), np.ones_like(scores_anomaly_2)])\n",
    "y_scores = np.concatenate([scores_test_2, scores_anomaly_2])\n",
    "\n",
    "# === ROC Curve ===\n",
    "fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "roc_fig = go.Figure()\n",
    "roc_fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name='ROC Curve'))\n",
    "roc_fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', line=dict(dash='dash')))\n",
    "roc_fig.update_layout(title=f'ROC Curve (AUC = {roc_auc:.4f})',\n",
    "                      xaxis_title='False Positive Rate',\n",
    "                      yaxis_title='True Positive Rate')\n",
    "\n",
    "# === PR Curve ===\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "pr_fig = go.Figure()\n",
    "pr_fig.add_trace(go.Scatter(x=recall, y=precision, mode='lines', name='PR Curve'))\n",
    "pr_fig.update_layout(title=f'Precision-Recall Curve (AUC = {pr_auc:.4f})',\n",
    "                     xaxis_title='Recall',\n",
    "                     yaxis_title='Precision')\n",
    "\n",
    "# === Best threshold based on F1 Score ===\n",
    "thresholds = np.linspace(0, 1, 200)\n",
    "best_f1 = 0\n",
    "best_threshold = 0\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = (y_scores >= t).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = t\n",
    "\n",
    "# Final metrics\n",
    "final_preds = (y_scores >= best_threshold).astype(int)\n",
    "final_acc = accuracy_score(y_true, final_preds)\n",
    "final_prec = precision_score(y_true, final_preds)\n",
    "final_rec = recall_score(y_true, final_preds)\n",
    "\n",
    "# Show plots\n",
    "roc_fig.show()\n",
    "pr_fig.show()\n",
    "\n",
    "# Print best metrics\n",
    "print(f\"Best Threshold: {best_threshold:.4f}\")\n",
    "print(f\"Accuracy: {final_acc:.4f}\")\n",
    "print(f\"Precision: {final_prec:.4f}\")\n",
    "print(f\"Recall: {final_rec:.4f}\")\n",
    "print(f\"F1 Score: {best_f1:.4f}\")\n"
   ],
   "id": "fdc4f721806fc1ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6384fcc919371915",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import seaborn as sns\n",
    "gmm = GaussianMixture(n_components=5, covariance_type='full').fit(mu_train)\n",
    "train_gmm_scores = gmm.score_samples(mu_train)\n",
    "test_gmm_scores = gmm.score_samples(mu_test)\n",
    "ano_gmm_scores = gmm.score_samples(mu_anomaly)\n",
    "\n",
    "sns.boxplot(data=[train_gmm_scores, test_gmm_scores, ano_gmm_scores])\n",
    "\n",
    "#Setting labels\n",
    "plt.xlabel('Class')\n",
    "plt.title('Gaussian Mixture scores')\n",
    "plt.xticks([0, 1, 2], ['Train', 'Test', 'Anomaly'])  # Label x-ticks with class names names\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "id": "49b81be368f06dde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3138ad38efbeef33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute the mean vector\n",
    "mean_vector = np.mean(mu_train, axis=0)\n",
    "print(mean_vector.shape)\n",
    "\n",
    "# Compute the covariance matrix\n",
    "covariance_matrix = np.cov(mu_train, rowvar=False)\n",
    "\n",
    "epsilon = 1e-10\n",
    "regularized_cov_matrix = covariance_matrix + epsilon * np.eye(covariance_matrix.shape[0])\n",
    "\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.linalg import inv\n",
    "\n",
    "# Inverse of the covariance matrix\n",
    "inv_cov_matrix = inv(regularized_cov_matrix)\n",
    "\n",
    "# Function to calculate Mahalanobis distance\n",
    "def compute_mahalanobis_distance(data_point, mean_vector, inv_cov_matrix):\n",
    "    diff = data_point - mean_vector\n",
    "    distance = np.sqrt(diff.T @ inv_cov_matrix @ diff)\n",
    "    return distance\n",
    "\n",
    "# Compute Mahalanobis distances for all data points\n",
    "\n",
    "train_mahalanobis_distances = np.array([compute_mahalanobis_distance(point, mean_vector, inv_cov_matrix) for point in mu_train])\n",
    "test_mahalanobis_distances = np.array([compute_mahalanobis_distance(point, mean_vector, inv_cov_matrix) for point in mu_test])\n",
    "ano_mahalanobis_distances = np.array([compute_mahalanobis_distance(point, mean_vector, inv_cov_matrix) for point in mu_anomaly])\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create the box plot\n",
    "sns.boxplot(data=[train_mahalanobis_distances, test_mahalanobis_distances, ano_mahalanobis_distances])\n",
    "\n",
    "# Setting labels\n",
    "#plt.xlabel('Class')\n",
    "#plt.ylabel('cosine_similarity Scores')\n",
    "plt.title('Mahalanobis distance to mean latent vector of normal data')\n",
    "plt.xticks([0, 1, 2], ['Train', 'Test', 'Anomaly'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "3e7ba043c6c52962",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "\n",
    "\n",
    "# One-Class SVM\n",
    "ocsvm = OneClassSVM()\n",
    "ocsvm.fit(mu_train)\n",
    "\n",
    "train_svm_scores = ocsvm.score_samples(mu_train)\n",
    "test_svm_scores = ocsvm.score_samples(mu_test)\n",
    "ano_svm_scores = ocsvm.score_samples(mu_anomaly)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create the box plot\n",
    "sns.boxplot(data=[train_svm_scores, test_svm_scores, ano_svm_scores])\n",
    "\n",
    "# Setting labels\n",
    "#plt.xlabel('Class')\n",
    "#plt.ylabel('cosine_similarity Scores')\n",
    "plt.title('One-Class SVM scores')\n",
    "plt.xticks([0, 1, 2], ['Train', 'Test', 'Anomaly'])\n",
    "\n",
    "\n"
   ],
   "id": "1dbf52630a16f068",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "95589d959ff66034",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_mahalanobis = np.concatenate([test_mahalanobis_distances, ano_mahalanobis_distances])\n",
    "all_gmm = np.concatenate([test_gmm_scores, ano_gmm_scores])\n",
    "labels = np.concatenate([np.zeros_like(test_mahalanobis_distances), np.ones_like(ano_mahalanobis_distances)])"
   ],
   "id": "d2c6e6db2fc2c0af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import f1_score, precision_recall_curve, roc_curve, auc\n",
    "\n",
    "def best_f1_threshold(scores, labels, greater_is_anomaly=True):\n",
    "    if not greater_is_anomaly:\n",
    "        scores = -scores  # \n",
    "    precisions, recalls, thresholds = precision_recall_curve(labels, scores)\n",
    "    f1s = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
    "    best_idx = np.argmax(f1s)\n",
    "    return thresholds[best_idx], f1s[best_idx]\n",
    "\n",
    "# For Mahalanobis (higher distance = more anomalous)\n",
    "mahal_thresh, mahal_f1 = best_f1_threshold(all_mahalanobis, labels, greater_is_anomaly=True)\n",
    "\n",
    "# For GMM (typically, lower score = more anomalous)\n",
    "gmm_thresh, gmm_f1 = best_f1_threshold(all_gmm, labels, greater_is_anomaly=False)\n"
   ],
   "id": "d8c62f3dedad30de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ROC Curve\n",
    "fpr_mahal, tpr_mahal, _ = roc_curve(labels, all_mahalanobis)\n",
    "roc_auc_mahal = auc(fpr_mahal, tpr_mahal)\n",
    "\n",
    "fpr_gmm, tpr_gmm, _ = roc_curve(labels, -all_gmm)  # \n",
    "roc_auc_gmm = auc(fpr_gmm, tpr_gmm)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr_mahal, tpr_mahal, label=f\"Mahalanobis (AUC = {roc_auc_mahal:.4f})\")\n",
    "plt.plot(fpr_gmm, tpr_gmm, label=f\"GMM (AUC = {roc_auc_gmm:.4f})\")\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "prec_mahal, rec_mahal, _ = precision_recall_curve(labels, all_mahalanobis)\n",
    "prc_auc_mahal = auc(rec_mahal, prec_mahal)\n",
    "\n",
    "prec_gmm, rec_gmm, _ = precision_recall_curve(labels, -all_gmm)\n",
    "prc_auc_gmm = auc(rec_gmm, prec_gmm)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(rec_mahal, prec_mahal, label=f\"Mahalanobis (AUC = {prc_auc_mahal:.4f})\")\n",
    "plt.plot(rec_gmm, prec_gmm, label=f\"GMM (AUC = {prc_auc_gmm:.4f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "5136a6c860fe3dac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8b7bbc21d8293bc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this code beat is applied on the KL divergence part of the ELBO loss\n",
    "\n",
    "GMM and mahanolobis distance is trained on the mean latent but the log-variance latent space is also smooth and sperates the class"
   ],
   "id": "a2a0e8519b550b98"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
