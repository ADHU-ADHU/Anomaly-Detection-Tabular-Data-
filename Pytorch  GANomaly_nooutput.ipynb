{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "# Load the train.mat file\n",
    "mat_file_path = \"train.mat\"\n",
    "data = scipy.io.loadmat(mat_file_path)\n",
    "\n",
    "# Display the keys in the loaded data\n",
    "data.keys()\n"
   ],
   "id": "4038a321a3a3c9df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract the Train_data object\n",
    "train_data = data['Train_data']\n",
    "\n",
    "# Check the type and shape\n",
    "type(train_data), train_data.shape\n"
   ],
   "id": "80ae2e3275926b2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_data[0].size",
   "id": "ee4b4e4bdd25ba43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_data.shape[1]",
   "id": "8f72a9c1e2830ac9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract the first system (system 0)\n",
    "system_0 = train_data[0, 0]\n",
    "\n",
    "# Check what fields or attributes are available in this system's structure\n",
    "system_0.dtype.names\n"
   ],
   "id": "4c4b9656c99546d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "system_0",
   "id": "cc4a9715d5e2b8fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract trajectory and label for system 0\n",
    "trajectory_0 = system_0['trajectory']\n",
    "label_0 = system_0['Label']\n",
    "\n",
    "# Check shapes of both arrays\n",
    "trajectory_0.shape, label_0.shape\n"
   ],
   "id": "7c020db53b84e0f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trajectory_0[0,0].shape",
   "id": "7d9ba0b30eb845d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "label_0[0].shape\n",
   "id": "7f2d3451463c1fe6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract trajectory of component 0\n",
    "component_0_traj = trajectory_0[0, 0]  # 1st row, 1st component\n",
    "\n",
    "# Check shape and type\n",
    "type(component_0_traj), component_0_traj.shape\n"
   ],
   "id": "ac3d9731b6284f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_normal_data = []\n",
    "\n",
    "for i in range(train_data.shape[1]):\n",
    "    system = train_data[0, i]\n",
    "    trajectories = system['trajectory']  # shape (1, 4)\n",
    "    labels = system['Label']            # shape (4, 1000)\n",
    "    \n",
    "    for j in range(4):  # For each of the 4 trajectories per system\n",
    "        traj = trajectories[0, j]       # shape (10, 1000)\n",
    "        label = labels[j]               # shape (1000,)\n",
    "        \n",
    "        # Select only columns (time steps) where label is 0\n",
    "        normal_indices = label == 0\n",
    "        normal_data = traj[:, normal_indices]  # shape (10, N_normal)\n",
    "        \n",
    "        # Transpose to (N_normal, 10)\n",
    "        normal_data = normal_data.T\n",
    "        all_normal_data.append(normal_data)\n",
    "\n",
    "# Stack all normal data\n",
    "X_train = np.vstack(all_normal_data) \n",
    "print(\"Shape of normal trajectory: \" , X_train.shape)"
   ],
   "id": "dd8220a71c509a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "\n",
   "id": "c1a1859e05841084",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Collect all anomalous samples\n",
    "all_anomaly_data = []\n",
    "\n",
    "for i in range(train_data.shape[1]):\n",
    "    system = train_data[0, i]\n",
    "    trajectories = system['trajectory']  # shape (1, 4)\n",
    "    labels = system['Label']            # shape (4, 1000)\n",
    "    \n",
    "    for j in range(4):  # For each of the 4 trajectories per system\n",
    "        traj = trajectories[0, j]       # shape (10, 1000)\n",
    "        label = labels[j]               # shape (1000,)\n",
    "        \n",
    "        # Select only columns (time steps) where label is 1\n",
    "        anomaly_indices = label == 1\n",
    "        anomaly_data = traj[:, anomaly_indices]  # shape (10, N_anomaly)\n",
    "        \n",
    "        # Transpose to (N_anomaly, 10)\n",
    "        anomaly_data = anomaly_data.T\n",
    "        all_anomaly_data.append(anomaly_data)\n",
    "\n",
    "# Stack all anomaly data\n",
    "X_anomaly = np.vstack(all_anomaly_data)  # Final shape: (N_total_anomaly, 10)\n",
    "print(\"Anomaly data shape:\", X_anomaly.shape)"
   ],
   "id": "625bd37a34bb373d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ],
   "id": "57d95664a9cd2855",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device \", device)"
   ],
   "id": "b73e2bb3bbab1849",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train/test split\n",
    "train_df, test_df = train_test_split(\n",
    "    X_train,  # Ensure we only use the specified features\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ],
   "id": "329d248c8922d919",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Scale data with standard scaler\n",
    "#scaler = StandardScaler()\n",
    "#train_df_scaled = scaler.fit_transform(train_df)\n",
    "#test_df_scaled = scaler.transform(test_df)\n",
    "#anomaly_scaled = scaler.transform(X_anomaly)  \n",
    "\n",
    "# Scale data with min-max scaler\n",
    "#scaler = MinMaxScaler()\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "train_df_scaled = scaler.fit_transform(train_df)\n",
    "test_df_scaled = scaler.transform(test_df)\n",
    "anomaly_scaled = scaler.transform(X_anomaly)  "
   ],
   "id": "c50a911fcc63743a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"shape of train data \", train_df_scaled.shape)\n",
    "print(\"shape of test data \", test_df_scaled.shape)\n",
    "print(\"shape of anomaly data \", anomaly_scaled.shape)"
   ],
   "id": "d551dd6033e4dd31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert to PyTorch tensors\n",
    "train_data_tensor = torch.FloatTensor(train_df_scaled).to(device)\n",
    "test_data_tensor = torch.FloatTensor(test_df_scaled).to(device)\n",
    "anomaly_data_tensor = torch.FloatTensor(anomaly_scaled).to(device)\n",
    "\n",
    "\n",
    "# Step 3: GAN Architecture\n",
    "\n",
    "\n",
    "data_dim = train_data_tensor.shape[1]\n",
    "data_dim"
   ],
   "id": "28eb971fe201fee4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "latent_dim =8  # for encoder part of the \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, data_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(data_dim, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Linear(16, latent_dim),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, data_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, data_dim),\n",
    "            nn.Tanh()  # since scaled to [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n"
   ],
   "id": "b8d32f3ae4deffa5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c655e5c91492b44f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, data_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(data_dim, 32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ],
   "id": "35008222f55ee5b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9f9b523976ad26cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Encoder2(nn.Module):  # Same structure as Encoder1\n",
    "    def __init__(self, data_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(data_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128,64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64,32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(32,latent_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ],
   "id": "128faafdf3b1181c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 4: Instantiate models\n",
    "encoder = Encoder(data_dim, latent_dim).to(device)\n",
    "decoder = Decoder(data_dim, latent_dim).to(device)\n",
    "encoder2 = Encoder2(data_dim, latent_dim).to(device)\n",
    "\n",
    "discriminator = Discriminator(data_dim).to(device)\n",
    "\n",
    "loss_ad = nn.BCELoss()\n",
    "#loss_rec = nn.MSELoss()\n",
    "loss_rec = nn.L1Loss()\n",
    "loss_enc = nn.MSELoss()\n",
    "\n",
    "\n",
    "#lr = 0.0001"
   ],
   "id": "17ec3647252d6e16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#opt_g = optim.Adam(generator.parameters(), lr=lr)\n",
    "#opt_d = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "opt_g = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.0002,betas=(0.5, 0.999))\n",
    "opt_en2 = optim.Adam(encoder2.parameters(), lr=0.0002, betas=(0.5, 0.999) )\n",
    "opt_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999) )\n",
    "\n",
    "\n",
    "# Step 5: Training loop\n",
    "epochs = 150\n",
    "batch_size = 512\n",
    "\n",
    "# Weights for loss terms\n",
    "w_ad = 1\n",
    "w_rec = 50\n",
    "w_enc = 1  "
   ],
   "id": "f18dee8d74f5f7db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "dataset = TensorDataset(train_data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ],
   "id": "6fb73b9a3faf7f26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Setup for checkpointing ===\n",
    "checkpoint_path = 'checkpoints/'\n",
    "checkpoint_file = os.path.join(checkpoint_path, 'ganomaly_checkpoint.pth')\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "start_epoch = 0\n",
    "best_loss_d = float('inf')\n",
    "\n",
    "# === Try to load checkpoint if it exists ===\n",
    "if os.path.exists(checkpoint_file):\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "    encoder2.load_state_dict(checkpoint['encoder2_state_dict'])\n",
    "\n",
    "    opt_g.load_state_dict(checkpoint['opt_g_state_dict'])\n",
    "    opt_d.load_state_dict(checkpoint['opt_d_state_dict'])\n",
    "    opt_en2.load_state_dict(checkpoint['opt_en2_state_dict'])\n",
    "\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_loss_d = checkpoint['best_loss_d']\n",
    "\n",
    "    print(f\"✅ Loaded checkpoint from epoch {start_epoch - 1}\")\n",
    "\n",
    "# === Lists to store losses ===\n",
    "losses_d = []\n",
    "losses_g = []\n",
    "losses_rec = []\n",
    "losses_enc = []\n",
    "\n",
    "for epoch in tqdm(range(start_epoch, epochs)):\n",
    "    epoch_loss_d = 0.0\n",
    "    epoch_loss_g = 0.0\n",
    "    epoch_loss_rec = 0.0\n",
    "    epoch_loss_enc = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch_idx, (real_data,) in enumerate(dataloader):\n",
    "        num_batches += 1\n",
    "        real_data = real_data.to(device)\n",
    "\n",
    "        # === Train Discriminator ===\n",
    "        latent = encoder(real_data)\n",
    "        fake_data = decoder(latent).detach()\n",
    "\n",
    "        d_real = discriminator(real_data)\n",
    "        d_fake = discriminator(fake_data)\n",
    "\n",
    "        real_labels = torch.ones(real_data.size(0), 1, device=device)\n",
    "        fake_labels = torch.zeros(fake_data.size(0), 1, device=device)\n",
    "\n",
    "        loss_d_real = loss_ad(d_real, real_labels)\n",
    "        loss_d_fake = loss_ad(d_fake, fake_labels)\n",
    "        loss_d = (loss_d_real + loss_d_fake) / 2\n",
    "\n",
    "        opt_d.zero_grad()\n",
    "        loss_d.backward()\n",
    "        opt_d.step()\n",
    "\n",
    "        # === Train Generator (encoder + decoder) ===\n",
    "        latent = encoder(real_data)\n",
    "        fake_data = decoder(latent)\n",
    "        d_fake = discriminator(fake_data)\n",
    "\n",
    "        loss_ad_g = loss_ad(d_fake, real_labels)\n",
    "        loss_rec_g = loss_rec(fake_data, real_data)\n",
    "        loss_g = w_ad * loss_ad_g + w_rec * loss_rec_g\n",
    "\n",
    "        opt_g.zero_grad()\n",
    "        loss_g.backward()\n",
    "        opt_g.step()\n",
    "\n",
    "        # === Train Encoder2 ===\n",
    "        with torch.no_grad():\n",
    "            latent = encoder(real_data)\n",
    "            fake_data = decoder(latent)\n",
    "\n",
    "        latent_recon = encoder2(fake_data)\n",
    "        loss_enc_g = loss_enc(latent, latent_recon) * w_enc\n",
    "\n",
    "        opt_en2.zero_grad()\n",
    "        loss_enc_g.backward()\n",
    "        opt_en2.step()\n",
    "\n",
    "        # === Accumulate losses ===\n",
    "        epoch_loss_d += loss_d.item()\n",
    "        epoch_loss_g += loss_g.item()\n",
    "        epoch_loss_rec += loss_rec_g.item()\n",
    "        epoch_loss_enc += loss_enc_g.item()\n",
    "\n",
    "    # === Average Epoch Losses ===\n",
    "    avg_loss_d = epoch_loss_d / num_batches\n",
    "    avg_loss_g = epoch_loss_g / num_batches\n",
    "    avg_loss_rec = epoch_loss_rec / num_batches\n",
    "    avg_loss_enc = epoch_loss_enc / num_batches\n",
    "\n",
    "    losses_d.append(avg_loss_d)\n",
    "    losses_g.append(avg_loss_g)\n",
    "    losses_rec.append(avg_loss_rec)\n",
    "    losses_enc.append(avg_loss_enc)\n",
    "\n",
    "    # === Save checkpoint every N epochs or on best loss improvement ===\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'encoder_state_dict': encoder.state_dict(),\n",
    "            'decoder_state_dict': decoder.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'encoder2_state_dict': encoder2.state_dict(),\n",
    "            'opt_g_state_dict': opt_g.state_dict(),\n",
    "            'opt_d_state_dict': opt_d.state_dict(),\n",
    "            'opt_en2_state_dict': opt_en2.state_dict(),\n",
    "            'best_loss_d': best_loss_d,\n",
    "        }, checkpoint_file)\n",
    "        print(f\"💾 Checkpoint saved at epoch {epoch} with Loss D: {avg_loss_d:.4f}\")\n",
    "\n",
    "    # === Early Stopping ===\n",
    "    if avg_loss_d < 0.3:\n",
    "        print(f\"⛔ Early stopping at epoch {epoch} as Discriminator loss reached {avg_loss_d:.4f}\")\n",
    "        break\n",
    "\n",
    "    # === Logging ===\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"📘 Epoch {epoch}: Loss D = {avg_loss_d:.4f}, Loss G = {avg_loss_g:.4f}, Recon = {avg_loss_rec:.4f}, Latent = {avg_loss_enc:.4f}\")\n",
    "        \n"
   ],
   "id": "54fedfc6c1651344",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(y=losses_d, mode='lines', name='Discriminator Loss'))\n",
    "fig.add_trace(go.Scatter(y=losses_g, mode='lines', name='Generator Loss'))\n",
    "fig.add_trace(go.Scatter(y=losses_rec, mode='lines', name='Reconstruction Loss'))\n",
    "fig.add_trace(go.Scatter(y=losses_enc, mode='lines', name='Latent Loss'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Training Losses Over Epochs',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Loss',\n",
    "    legend_title='Loss Type',\n",
    "    template='plotly_white',\n",
    "\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ],
   "id": "45855a2c5b6826d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Discriminator Loss\n",
    "fig_d = go.Figure()\n",
    "fig_d.add_trace(go.Scatter(y=losses_d, mode='lines', name='Discriminator Loss'))\n",
    "fig_d.update_layout(title='Discriminator Loss', xaxis_title='Epoch', yaxis_title='Loss', template='plotly_dark')\n",
    "fig_d.show()\n",
    "\n",
    "# Generator Loss\n",
    "fig_g = go.Figure()\n",
    "fig_g.add_trace(go.Scatter(y=losses_g, mode='lines', name='Generator Loss'))\n",
    "fig_g.update_layout(title='Generator Loss', xaxis_title='Epoch', yaxis_title='Loss', template='plotly_dark')\n",
    "fig_g.show()\n",
    "\n",
    "# Reconstruction Loss\n",
    "fig_rec = go.Figure()\n",
    "fig_rec.add_trace(go.Scatter(y=losses_rec, mode='lines', name='Reconstruction Loss'))\n",
    "fig_rec.update_layout(title='Reconstruction Loss', xaxis_title='Epoch', yaxis_title='Loss', template='plotly_dark')\n",
    "fig_rec.show()\n",
    "\n",
    "# Latent Loss\n",
    "fig_enc = go.Figure()\n",
    "fig_enc.add_trace(go.Scatter(y=losses_enc, mode='lines', name='Latent Loss'))\n",
    "fig_enc.update_layout(title='Latent Loss', xaxis_title='Epoch', yaxis_title='Loss', template='plotly_dark')\n",
    "fig_enc.show()\n"
   ],
   "id": "11601e6bbb8d8378",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "11d836a3890add7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "checkpoint = torch.load('checkpoints/ganomaly_checkpoint.pth')\n",
    "\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "encoder2.load_state_dict(checkpoint['encoder2_state_dict'])\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "discriminator.eval()\n",
    "encoder2.eval()\n"
   ],
   "id": "d16e7cfe2effbc07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e26a5b9a7680cea2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_scores(data_tensor):\n",
    "    with torch.no_grad():\n",
    "        z = encoder(data_tensor)\n",
    "        x_recon = decoder(z)\n",
    "        z_recon = encoder2(x_recon)\n",
    "\n",
    "        # Discriminator output (lower is more fake)\n",
    "        disc_score = discriminator(data_tensor).squeeze()  # shape: (N,)\n",
    "\n",
    "        # Reconstruction error (L1)\n",
    "        recon_score = torch.mean(torch.abs(data_tensor - x_recon), dim=1)\n",
    "\n",
    "        # Latent error (L2)\n",
    "        latent_score = torch.mean((z - z_recon) ** 2, dim=1)\n",
    "\n",
    "        # Combined score\n",
    "        total_score = recon_score + latent_score\n",
    "\n",
    "        return disc_score.cpu().numpy(), recon_score.cpu().numpy(), latent_score.cpu().numpy(), total_score.cpu().numpy()\n"
   ],
   "id": "d8b670bd0c8e9f8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scores_train = compute_scores(train_data_tensor)\n",
    "scores_test = compute_scores(test_data_tensor)\n",
    "scores_anomaly = compute_scores(anomaly_data_tensor)\n",
    "\n"
   ],
   "id": "4224c50f1561c2b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Score': list(scores_train[0]) + list(scores_test[0]) + list(scores_anomaly[0]),\n",
    "    'Set': (['Train'] * len(scores_train[0])) +\n",
    "           (['Test'] * len(scores_test[0])) +\n",
    "           (['Anomaly'] * len(scores_anomaly[0]))\n",
    "})\n",
    "\n",
    "fig = px.box(df, x='Set', y='Score', title='Discriminator Scores', template='plotly_dark')\n",
    "fig.show()\n"
   ],
   "id": "c6f60f4e3f98ae74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame({\n",
    "    'Score': list(scores_train[1]) + list(scores_test[1]) + list(scores_anomaly[1]),\n",
    "    'Set': (['Train'] * len(scores_train[1])) +\n",
    "           (['Test'] * len(scores_test[1])) +\n",
    "           (['Anomaly'] * len(scores_anomaly[1]))\n",
    "})\n",
    "\n",
    "fig = px.box(df, x='Set', y='Score', title='Reconstruction (L1) Scores', template='plotly_dark')\n",
    "fig.show()\n"
   ],
   "id": "1d24d0ddaaaf9d18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame({\n",
    "    'Score': list(scores_train[2]) + list(scores_test[2]) + list(scores_anomaly[2]),\n",
    "    'Set': (['Train'] * len(scores_train[2])) +\n",
    "           (['Test'] * len(scores_test[2])) +\n",
    "           (['Anomaly'] * len(scores_anomaly[2]))\n",
    "})\n",
    "\n",
    "fig = px.box(df, x='Set', y='Score', title='Latent (L2) Scores', template='plotly_dark')\n",
    "fig.show()\n"
   ],
   "id": "ecd27f505a523d98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame({\n",
    "    'Score': list(scores_train[3]) + list(scores_test[3]) + list(scores_anomaly[3]),\n",
    "    'Set': (['Train'] * len(scores_train[3])) +\n",
    "           (['Test'] * len(scores_test[3])) +\n",
    "           (['Anomaly'] * len(scores_anomaly[3]))\n",
    "})\n",
    "\n",
    "fig = px.box(df, x='Set', y='Score', title='Combined (L1 + L2) Scores', template='plotly_dark')\n",
    "fig.show()\n"
   ],
   "id": "dccc0038e52848df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, precision_recall_curve, auc,\n",
    "    accuracy_score, f1_score, precision_score, recall_score\n",
    ")\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Replace these with your actual scores\n",
    "scores_train_2 = np.array(scores_train[2])\n",
    "scores_test_2 = np.array(scores_test[2])\n",
    "scores_anomaly_2 = np.array(scores_anomaly[2])\n",
    "\n",
    "# Test data is normal (label 0), anomaly is (label 1)\n",
    "y_true = np.concatenate([np.zeros_like(scores_test_2), np.ones_like(scores_anomaly_2)])\n",
    "y_scores = np.concatenate([scores_test_2, scores_anomaly_2])\n",
    "\n",
    "# === ROC Curve ===\n",
    "fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "roc_fig = go.Figure()\n",
    "roc_fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name='ROC Curve'))\n",
    "roc_fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', line=dict(dash='dash')))\n",
    "roc_fig.update_layout(title=f'ROC Curve (AUC = {roc_auc:.4f})',\n",
    "                      xaxis_title='False Positive Rate',\n",
    "                      yaxis_title='True Positive Rate')\n",
    "\n",
    "# === PR Curve ===\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "pr_fig = go.Figure()\n",
    "pr_fig.add_trace(go.Scatter(x=recall, y=precision, mode='lines', name='PR Curve'))\n",
    "pr_fig.update_layout(title=f'Precision-Recall Curve (AUC = {pr_auc:.4f})',\n",
    "                     xaxis_title='Recall',\n",
    "                     yaxis_title='Precision')\n",
    "\n",
    "# === Best threshold based on F1 Score ===\n",
    "thresholds = np.linspace(0, 1, 200)\n",
    "best_f1 = 0\n",
    "best_threshold = 0\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = (y_scores >= t).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = t\n",
    "\n",
    "# Final metrics\n",
    "final_preds = (y_scores >= best_threshold).astype(int)\n",
    "final_acc = accuracy_score(y_true, final_preds)\n",
    "final_prec = precision_score(y_true, final_preds)\n",
    "final_rec = recall_score(y_true, final_preds)\n",
    "\n",
    "# Show plots\n",
    "roc_fig.show()\n",
    "pr_fig.show()\n",
    "\n",
    "# Print best metrics\n",
    "print(f\"Best Threshold: {best_threshold:.4f}\")\n",
    "print(f\"Accuracy: {final_acc:.4f}\")\n",
    "print(f\"Precision: {final_prec:.4f}\")\n",
    "print(f\"Recall: {final_rec:.4f}\")\n",
    "print(f\"F1 Score: {best_f1:.4f}\")\n"
   ],
   "id": "d823505960ba1dae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e3a48de13f055422",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, precision_recall_curve, auc,\n",
    "    accuracy_score, f1_score, precision_score, recall_score\n",
    ")\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Replace these with your actual scores\n",
    "scores_train_2 = np.array(scores_train[3])\n",
    "scores_test_2 = np.array(scores_test[3])\n",
    "scores_anomaly_2 = np.array(scores_anomaly[3])\n",
    "\n",
    "# Test data is normal (label 0), anomaly is (label 1)\n",
    "y_true = np.concatenate([np.zeros_like(scores_test_2), np.ones_like(scores_anomaly_2)])\n",
    "y_scores = np.concatenate([scores_test_2, scores_anomaly_2])\n",
    "\n",
    "# === ROC Curve ===\n",
    "fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "roc_fig = go.Figure()\n",
    "roc_fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name='ROC Curve'))\n",
    "roc_fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', line=dict(dash='dash')))\n",
    "roc_fig.update_layout(title=f'ROC Curve (AUC = {roc_auc:.4f})',\n",
    "                      xaxis_title='False Positive Rate',\n",
    "                      yaxis_title='True Positive Rate')\n",
    "\n",
    "# === PR Curve ===\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "pr_fig = go.Figure()\n",
    "pr_fig.add_trace(go.Scatter(x=recall, y=precision, mode='lines', name='PR Curve'))\n",
    "pr_fig.update_layout(title=f'Precision-Recall Curve (AUC = {pr_auc:.4f})',\n",
    "                     xaxis_title='Recall',\n",
    "                     yaxis_title='Precision')\n",
    "\n",
    "# === Best threshold based on F1 Score ===\n",
    "thresholds = np.linspace(0, 1, 200)\n",
    "best_f1 = 0\n",
    "best_threshold = 0\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = (y_scores >= t).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = t\n",
    "\n",
    "# Final metrics\n",
    "final_preds = (y_scores >= best_threshold).astype(int)\n",
    "final_acc = accuracy_score(y_true, final_preds)\n",
    "final_prec = precision_score(y_true, final_preds)\n",
    "final_rec = recall_score(y_true, final_preds)\n",
    "\n",
    "# Show plots\n",
    "roc_fig.show()\n",
    "pr_fig.show()\n",
    "\n",
    "# Print best metrics\n",
    "print(f\"Best Threshold: {best_threshold:.4f}\")\n",
    "print(f\"Accuracy: {final_acc:.4f}\")\n",
    "print(f\"Precision: {final_prec:.4f}\")\n",
    "print(f\"Recall: {final_rec:.4f}\")\n",
    "print(f\"F1 Score: {best_f1:.4f}\")\n"
   ],
   "id": "27d34dfc1c982417",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ed922333b37c3614",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_latents(encoder, data_tensor, batch_size=1024):\n",
    "    latents = []\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_tensor.size(0), batch_size):\n",
    "            batch = data_tensor[i:i + batch_size]\n",
    "            latent = encoder(batch)\n",
    "            latents.append(latent.cpu().numpy())\n",
    "    return np.concatenate(latents, axis=0)\n",
    "\n",
    "latent_train_1 = get_latents(encoder, train_data_tensor)\n",
    "latent_test_1 = get_latents(encoder, test_data_tensor)\n",
    "latent_anomaly_1 = get_latents(encoder, anomaly_data_tensor)\n",
    "\n",
    "latent_train_2 = get_latents(encoder2, train_data_tensor)\n",
    "latent_test_2 = get_latents(encoder2, test_data_tensor)\n",
    "latent_anomaly_2 = get_latents(encoder2, anomaly_data_tensor)\n"
   ],
   "id": "b33482bc489b820c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def downsample(arr, n=5000):\n",
    "    idx = np.random.choice(len(arr), size=min(n, len(arr)), replace=False)\n",
    "    return arr[idx]\n",
    "\n",
    "# Choose one encoder’s output to visualize (or concatenate both if you want combined)\n",
    "latent_train = downsample(latent_train_1)\n",
    "latent_test = downsample(latent_test_1)\n",
    "latent_anomaly = downsample(latent_anomaly_1)\n",
    "\n",
    "X = np.vstack([latent_train, latent_test, latent_anomaly])\n",
    "labels = (['train'] * len(latent_train) +\n",
    "          ['test'] * len(latent_test) +\n",
    "          ['anomaly'] * len(latent_anomaly))\n"
   ],
   "id": "50c6b0fff383eeaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "218cb1025fb9f60b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=5)\n",
    "X_2d = tsne.fit_transform(X)\n"
   ],
   "id": "c4fbfcb2153c02a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(X_2d, columns=['x', 'y'])\n",
    "df['label'] = labels\n",
    "\n",
    "fig = px.scatter(df, x='x', y='y', color='label',\n",
    "                 title='t-SNE Visualization of Encoder Latent Space')\n",
    "fig.show()\n"
   ],
   "id": "b32f98f3b3d03a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b45df8b279f0e83b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tsne = TSNE(n_components=3, perplexity=30, random_state=5)\n",
    "X_3d = tsne.fit_transform(X)\n",
    "\n",
    "df_3d = pd.DataFrame(X_3d, columns=['x', 'y', 'z'])\n",
    "df_3d['label'] = labels\n",
    "\n",
    "fig_3d = px.scatter_3d(df_3d, x='x', y='y', z='z', color='label',\n",
    "                       title='3D t-SNE Visualization of Encoder Latent Space')\n",
    "fig_3d.show()\n",
    "\n"
   ],
   "id": "be5972f5d5d590ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e329998f26457dbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Choose one encoder’s output to visualize (or concatenate both if you want combined)\n",
    "latent_train_2 = downsample(latent_train_2)\n",
    "latent_test_2 = downsample(latent_test_2)\n",
    "latent_anomaly_2 = downsample(latent_anomaly_2)\n",
    "\n",
    "X = np.vstack([latent_train_2, latent_test_2, latent_anomaly_2])\n",
    "labels = (['train'] * len(latent_train_2) +\n",
    "          ['test'] * len(latent_test_2) +\n",
    "          ['anomaly'] * len(latent_anomaly_2))\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=5)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "\n",
    "df = pd.DataFrame(X_2d, columns=['x', 'y'])\n",
    "df['label'] = labels\n",
    "\n",
    "fig = px.scatter(df, x='x', y='y', color='label',\n",
    "                 title='t-SNE Visualization of Encoder Latent Space')\n",
    "fig.show()\n"
   ],
   "id": "2947ce38ad75db00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tsne = TSNE(n_components=3, perplexity=30, random_state=5)\n",
    "X_3d = tsne.fit_transform(X)\n",
    "\n",
    "\n",
    "df_3d = pd.DataFrame(X_3d, columns=['x', 'y', 'z'])\n",
    "df_3d['label'] = labels\n",
    "\n",
    "fig_3d = px.scatter_3d(df_3d, x='x', y='y', z='z', color='label',\n",
    "                       title='3D t-SNE Visualization of Encoder Latent Space')\n",
    "fig_3d.show()"
   ],
   "id": "d66378d027d7a80d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": " ",
   "id": "71acb7e2f8d1ee19",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
